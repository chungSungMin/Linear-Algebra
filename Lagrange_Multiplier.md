우리가 최적화된 값을 찾기 위해서 간단하게 $f'(x) = 0$ 이되는 x를 찾았습니다. 그 이유는 바로 $f'(x) = 0$ 이 최적화의 필요조건이기 떄문입니다.

convex 문제가 아니더라도 어떠한 문제가 오더라도 최적의 값은 반드시 gradient = 0이 지점에서 최적의 값을 얻을 수 있습니다. 그래서 우리는 계속해서 $f'(x) = 0$ 을 찾았습니다. 즉 최소한의 필요조건이라도 찾아야 되기 떄문입니다.

<img width="284" alt="image" src="https://github.com/user-attachments/assets/8aad0cb4-9490-400c-b7cf-63533f38b1f3" />

<br>

그렇다면 우리가 궁금해 하는 특정 조건이 있는 문제에서도 단순히 $f'(x) = 0$ 을 만족하는 x가 필요조건일 까요?? 

<img width="328" alt="image" src="https://github.com/user-attachments/assets/98ca35e4-ea0d-4976-b703-b956c63dcc4a" />

<br>


위의 그래프에서 만일 파란그래프의 최소값을 구하는데, 조건으로 빨간 그래프 보다 크거나 작은 경우라고 해봅시다. 이때의 해는 $f'(x) = 0$ 을 만족하는 곳이 solution이 아닙니다. 그래서 다음과 같이 특정 조건이 있는 경우 최적화의 필요조건이 달라지게 됩니다.

조건이 있는 문제에서 최적화의 필요조건을 제공하는 것이 바로 라그랑주 승수법입니다.

라그랑주 승수법의 식은 아래와 같습니다.

$$
L(x,\lambda) = f(x) + \lambda h(x)
$$

여기서 f(x) 함수는 우리가 최적화를 하고 싶은 함수이고, h(x) 함수는 특정 조건을 의미하게됩니다.

이를 해석해보면 아래의 2가지 가정이 바로 조건이 있는 경우의 필요조건이 됨을 확인할 수 있습니다.

1. $f'(x^*) + \lambda h(x^*) = 0$ 을 만족하는 x가 존재한다.
2. $h(x^*) = 0$  을 만족한다

즉, 최적의 값에서 f(x)의 기울기와 h(x)의 기울기가 동일해야함을 의미합니다.

이제 기하학적으로 왜 이 2가지 가정이 조건이 있는 경우 최적화의 방법인지 알아봅시다.

<br>

<img width="654" alt="image" src="https://github.com/user-attachments/assets/6a38f9b2-a9db-4487-b07e-cefcc5b0cb69" />


<br>

위와 같이 h(x)와 f(x)가 평행 한경우, 우리가 $h(x^*) = 0$ 을 만족하면서 f(x)의 값을 값을 줄이는 방향으로 가기 위해서는  영역2로 가야합니다. 하지만 영역2로 가게 된다면 결국 $h(x^*) != 0$  이기에 h(x)의 값이 달라지게 됩니다. 결국 1번과 2번을 만족하게 되면 해당 x가 바로 optimum이라는 것을 의미합니다. 왜냐하면 더이상 f(x)의 gradient를 줄일수 없기 떄문입니다. 1번과 2번을 모두 만족하기 위해서는 u vector 방향으로 가는 방향뿐인데 이는 f(x)의 gradient를 더이상 줄일수 없습니다. 즉 더이상 줄일수 없는 Local Optimum임을 의미하게 됩니다.

<br>
그렇다면 만일 1번 조건이 없다면 어떻게 될까요??

<br>
<img width="659" alt="image" src="https://github.com/user-attachments/assets/f8b070e1-577c-4a2c-8f01-8ddcdc62392c" />


<br>

다음과 같이 h(x) = 0 을 만족하기 위해서 h(x)의 graident와 수직인 방향으로 이동하게 되면 2번을 만족하면서 f(x)의 gradient를 더 줄일 수 있습니다. 하지만 f(x)의 gradient를 줄일수 있다는 것은 결국 해당 위치가 Local optimum이 아니라는것을 의미하게 됩니다.

그래서 우리는 이러한 기하학적 해석을 통해서 아래 2조건을 만족하는 라그랑주 승수법을 조건이 있는 최적화 방법에서 사용하게 된것입니다.

$$
L(x,\lambda) = f(x) + \lambda h(x)
$$

이를 푸는 방법은 아래와 같이 2개를 풀면됩니다.

$$
{{\nabla L(x, \lambda)} \over {\nabla x} } = 0 
$$

$$
{{\nabla L(x, \lambda)} \over {\nabla \lambda} } = 0 
$$

위 2개를 만족하는 연립 방정식을 풀게되면 최적의 값을 얻게 됩니다.

그리고 이렇게 해서 얻어진 $\lambda$는 제약조건이 최적화 함수에 미치는 영향을 의미합니다. 만일 $\lambda$ 값이 크다면 조건이 조금 변경될 경우 최적화 값에 많은 영향을 미친다는 것을 의미하게 됩니다.

즉, 라그랑주 승수법을 통해 얻은 최적의 변수  x 는 제약 조건 하에서 목적 함수를 최적화하는 해를 제공하며, 라그랑주 승수  \lambda 는 해당 제약 조건이 최적화 문제에 미치는 영향을 정량적으로 나타냅니다.
